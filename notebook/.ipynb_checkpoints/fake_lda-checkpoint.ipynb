{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('fake.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=12999, step=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text = data[['text']]\n",
    "data_text.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data_text[\"index\"] = data_text.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12999\n",
      "                                                text  index\n",
      "0  Print They should pay all the back all the mon...      0\n",
      "1  Why Did Attorney General Loretta Lynch Plead T...      1\n",
      "2  Red State : \\nFox News Sunday reported this mo...      2\n",
      "3  Email Kayla Mueller was a prisoner and torture...      3\n",
      "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vidya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U genism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Common', 'Sense', 'as', 'defined', 'by', 'the', 'Cambridge', 'Academic', 'Text', 'Dictionary', 'is', 'the', 'ability', 'to', 'use', 'good', 'judgement', 'in', 'making', 'decisions', 'to', 'live', 'in', 'a', 'reasonable', 'and', 'safe', 'way.', 'While', 'that', 'may', 'have', 'sounded', 'reasonable', 'and', 'safe', 'back', 'in', 'the', 'sixties', 'and', 'seventies', 'when', 'I', 'was', 'growing', 'up,', 'I', 'find', 'myself', 'at', 'risk', 'daily', 'based', 'on', 'the', 'judgement', 'of', 'others', 'who', 'make', 'decisions', 'that', 'have', 'lasting', 'and', 'irreversible', 'consequences.', 'Ever', 'since', 'I', 'started', 'writing', 'sporadically,', 'beginning', 'in', 'the', 'early', 'eighties', 'for', 'small', 'newspapers', 'like', 'the', 'Chicago', 'Daily', 'News', 'and', 'the', 'Chicago', 'Defender,', 'I', 'have', 'enjoyed', 'the', 'opportunity', 'to', 'research,', 'find', 'and', 'expose', 'resources.', 'I', 'started', 'with', '“Canons', 'on', 'Careers”,', 'which', 'was', 'my', 'weekly', 'contribution', 'to', 'those', 'readers', 'who', 'were', 'looking', 'for', 'career', 'and', 'employment', 'opportunities', 'and', 'financial', 'aid', 'to', 'pay', 'for,', 'and', 'in', 'some', 'cases', 'get', 'paid', 'for', 'attending', 'college.', 'When', 'I', 'started', '“Just', 'Saying”', 'for', 'the', 'Guardian', 'Liberty', 'Voice', 'in', '2014,', 'I', 'was', 'motivated', 'to', 'express', 'my', 'personal', 'opinions', 'about', 'the', 'world', 'around', 'me', 'as', 'I', 'saw', 'it', 'through', 'my', 'limited', 'world', 'view.', 'As', 'I', 'see', 'the', 'local', 'landscape', 'that', 'I', 'travail', 'everyday,', 'and', 'the', 'world', 'at', 'large', 'portrayed', 'to', 'me', 'and', 'others', 'by', 'media', 'moguls', 'and', 'highly', 'paid', 'infotainment', 'charlatans,', 'I', 'write', 'to', 'vent', 'my', 'frustrations', 'and', 'fears', 'that', 'the', 'world', 'around', 'me', 'has', 'been', 'dumb', 'down', 'to', 'the', 'point', 'where', 'Common', 'Sense', 'will', 'never', 'again', 'be', 'Common!\\nSince', 'I', 'grew', 'up', 'in', 'public', 'housing', 'and', 'went', 'to', 'public', 'schools,', 'I', 'have', 'never', 'really', 'thought', 'of', 'myself', 'as', 'an', 'intellectual', 'or', 'the', 'brightest', 'candle', 'on', 'the', 'cake.', 'I', 'never', 'ever', 'thought', 'that', 'those', 'around', 'me', 'would', 'fall', 'to', 'such', 'a', 'level', 'of', 'illiteracy', 'where', 'the', 'adults', 'and', 'the', 'majority', 'of', 'inner', 'city', 'youth', 'and', 'young', 'adults', 'would', 'struggle', 'to', 'read', 'and', 'write', 'at', 'an', 'eight', 'grade', 'level', 'or', 'higher.', 'I', 'am', 'in', 'the', 'business', 'of', 'hiring', 'people', 'daily', 'and', '7', 'out', 'of', 'ten', 'people', 'who', 'I', 'put', 'to', 'work', 'daily', 'cannot', 'properly', 'fill', 'out', 'an', 'application', 'or', 'transfer', 'information', 'from', 'a', 'Resume’', 'to', 'the', 'application.', 'The', 'fact', 'that', 'illiteracy', 'has', 'been', 'broken', 'down', 'to', 'three', 'categories,', 'functional,', 'cultural', 'and', 'moral', 'doesn’t', 'ease', 'my', 'anxieties', 'as', 'I', 'see', 'relatively', 'modest', 'and', 'simple', 'differences', 'of', 'opinion', 'escalate', 'to', 'levels', 'of', 'violence', 'and', 'idiocy', 'that', 'cost', 'people', 'their', 'limbs', 'and', 'their', 'lives.', 'While', 'Functional', 'illiteracy', 'plagues', 'over', '24', 'million', 'Americans,', 'with', 'over', 'a', 'million', 'of', 'them', 'graduating', 'from', 'high', 'school', 'with', 'a', 'diploma', 'the', 'value', 'of', 'a', 'wet', 'food', 'stamp,', 'Cultural', 'illiteracy', 'is', 'the', 'lack', 'of', 'knowledge', 'of', 'the', 'significant', 'historical', 'events', 'that', 'shaped', 'this', 'country', 'which', 'obscures', 'the', 'lessons', 'learned', '(', 'or', 'Not', 'learned', ')!', 'Moral', 'illiteracy', 'completes', 'the', 'three', 'forms', 'of', 'illiteracy,', 'and', 'to', 'my', 'view', 'causes', 'the', 'senseless', 'and', 'heinous', 'callousness', 'that', 'prevails', 'in', 'the', 'United', 'States', 'today!', 'Our', 'young', 'are', 'not', 'taught', 'the', 'moral', 'values', 'of', 'the', 'old', 'and', 'therefore', 'disrespect', 'the', 'old,', 'value', 'the', 'new', 'and', 'care', 'more', 'about', 'the', 'self', 'than', 'the', 'community', 'of', 'you!', 'That', 'said,', 'how', 'could', 'common', 'sense', 'ever', 'factor', 'into', 'the', 'equation', '!', 'There', 'is', 'no', 'ME', 'in', 'common', 'and', 'the', 'only', 'sense', 'that', 'counts', 'is', 'mine!\\nI’m', 'Just', 'Saying……What’s', 'a', 'fella', 'to', 'do?', 'I', 'live', 'in', 'a', 'country', 'that', 'just', 'elected', 'a', 'guy', 'named', 'Donald', 'Trump,', 'President,', 'who', 'says', 'he', 'represents', 'the', 'people', 'and', 'that', 'he', 'is', 'the', 'change', 'agent', 'that', 'is', 'going', 'to', 'make', 'America', 'GREAT', 'again.', 'Yes,', 'this', 'is', 'the', 'guy', 'who', 'going', 'to', 'bring', 'back', 'all', 'the', 'jobs', 'that', 'our', 'poor', 'trade', 'polices', 'gave', 'away.', 'Yes,', 'this', 'is', 'the', 'guy', 'who', 'says', 'Climate', 'Change', 'is', 'a', 'hoax', 'and', 'all', 'those', 'coal', 'mining', 'jobs', 'are', 'coming', 'back', 'to', 'West', 'Virginia,', 'Pennsylvania', 'and', 'all', 'those', 'lily', 'white', 'towns', 'who', 'used', 'to', 'thrive', 'on', 'those', 'deadly', 'but', 'well', 'paying', 'jobs.', 'This', 'is', 'also', 'the', 'guy', 'who', 'is', 'the', 'protege’', 'of', 'Roy', 'Cohn', '(the', 'instigator', 'and', 'arbitrator', 'of', 'McCarthyism)', 'who', 'caused', 'many', 'people', 'to', 'commit', 'suicide', 'and', 'destroyed', 'countless', 'lives', 'with', 'tactics', 'of', 'fear', 'using', 'race,', 'misogyny', 'and', 'communism!', 'In', 'a', 'NUTshell,', 'Donald', 'Trump', 'cohnned', 'many', 'of', 'America’s', 'so-called', 'common', 'folk', 'into', 'thinking', 'he', 'was', 'with', 'them', 'and', 'one', 'of', 'them!', 'Yep,', 'a', 'millionaire/billionaire', '(who', 'knows,', 'he', 'never', 'revealed', 'his', 'taxes)', 'who', 'claims', 'to', 'be', 'a', 'commoner!', 'It', 'took', 'me', 'a', 'long', 'time', 'Dad', 'but', 'now', 'I', 'get', 'it…….Common', 'Sense', 'is', 'not', 'Common!!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['common', 'sens', 'defin', 'cambridg', 'academ', 'text', 'dictionari', 'abil', 'good', 'judgement', 'make', 'decis', 'live', 'reason', 'safe', 'sound', 'reason', 'safe', 'sixti', 'seventi', 'grow', 'risk', 'daili', 'base', 'judgement', 'decis', 'last', 'irrevers', 'consequ', 'start', 'write', 'sporad', 'begin', 'earli', 'eighti', 'small', 'newspap', 'like', 'chicago', 'daili', 'news', 'chicago', 'defend', 'enjoy', 'opportun', 'research', 'expos', 'resourc', 'start', 'canon', 'career', 'week', 'contribut', 'reader', 'look', 'career', 'employ', 'opportun', 'financi', 'case', 'pay', 'attend', 'colleg', 'start', 'say', 'guardian', 'liberti', 'voic', 'motiv', 'express', 'person', 'opinion', 'world', 'limit', 'world', 'view', 'local', 'landscap', 'travail', 'everyday', 'world', 'larg', 'portray', 'media', 'mogul', 'high', 'pay', 'infotain', 'charlatan', 'write', 'vent', 'frustrat', 'fear', 'world', 'dumb', 'point', 'common', 'sens', 'common', 'grow', 'public', 'hous', 'go', 'public', 'school', 'think', 'intellectu', 'brightest', 'candl', 'cake', 'think', 'fall', 'level', 'illiteraci', 'adult', 'major', 'inner', 'citi', 'youth', 'young', 'adult', 'struggl', 'read', 'write', 'grade', 'level', 'higher', 'busi', 'hire', 'peopl', 'daili', 'peopl', 'work', 'daili', 'proper', 'applic', 'transfer', 'inform', 'resum', 'applic', 'fact', 'illiteraci', 'break', 'categori', 'function', 'cultur', 'moral', 'eas', 'anxieti', 'relat', 'modest', 'simpl', 'differ', 'opinion', 'escal', 'level', 'violenc', 'idioci', 'cost', 'peopl', 'limb', 'live', 'function', 'illiteraci', 'plagu', 'million', 'american', 'million', 'graduat', 'high', 'school', 'diploma', 'valu', 'food', 'stamp', 'cultur', 'illiteraci', 'lack', 'knowledg', 'signific', 'histor', 'event', 'shape', 'countri', 'obscur', 'lesson', 'learn', 'learn', 'moral', 'illiteraci', 'complet', 'form', 'illiteraci', 'view', 'caus', 'senseless', 'heinous', 'callous', 'prevail', 'unit', 'state', 'today', 'young', 'teach', 'moral', 'valu', 'disrespect', 'valu', 'care', 'self', 'communiti', 'say', 'common', 'sens', 'factor', 'equat', 'common', 'sens', 'count', 'say', 'fella', 'live', 'countri', 'elect', 'name', 'donald', 'trump', 'presid', 'say', 'repres', 'peopl', 'chang', 'agent', 'go', 'america', 'great', 'go', 'bring', 'job', 'poor', 'trade', 'polic', 'give', 'away', 'say', 'climat', 'chang', 'hoax', 'coal', 'mine', 'job', 'come', 'west', 'virginia', 'pennsylvania', 'lili', 'white', 'town', 'thrive', 'dead', 'pay', 'job', 'proteg', 'cohn', 'instig', 'arbitr', 'mccarthyism', 'caus', 'peopl', 'commit', 'suicid', 'destroy', 'countless', 'live', 'tactic', 'fear', 'race', 'misogyni', 'communism', 'nutshel', 'donald', 'trump', 'cohn', 'america', 'call', 'common', 'folk', 'think', 'millionair', 'billionair', 'know', 'reveal', 'tax', 'claim', 'common', 'take', 'long', 'time', 'common', 'sens', 'common']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_docs = documents['text'].map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [print, money, plus, entir, famili, come, need...\n",
       "1    [attorney, general, loretta, lynch, plead, fif...\n",
       "2    [state, news, sunday, report, morn, anthoni, w...\n",
       "3    [email, kayla, mueller, prison, tortur, isi, c...\n",
       "4    [email, healthcar, reform, america, great, mar...\n",
       "5    [print, hillari, go, absolut, berserk, explod,...\n",
       "6    [break, nypd, readi, arrest, weiner, case, hil...\n",
       "7    [break, nypd, readi, arrest, weiner, case, hil...\n",
       "8    [limbaugh, say, revel, wikileak, materi, start...\n",
       "9    [email, peopl, sick, evil, stop, law, mean, me...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 asap\n",
      "1 benefit\n",
      "2 bust\n",
      "3 case\n",
      "4 come\n",
      "5 commit\n",
      "6 control\n",
      "7 deport\n",
      "8 entir\n",
      "9 famili\n",
      "10 fraud\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (24, 1),\n",
       " (36, 1),\n",
       " (79, 1),\n",
       " (106, 2),\n",
       " (119, 1),\n",
       " (125, 4),\n",
       " (126, 3),\n",
       " (132, 1),\n",
       " (134, 1),\n",
       " (151, 1),\n",
       " (152, 1),\n",
       " (160, 1),\n",
       " (167, 1),\n",
       " (191, 2),\n",
       " (195, 1),\n",
       " (199, 1),\n",
       " (206, 2),\n",
       " (211, 2),\n",
       " (214, 1),\n",
       " (216, 1),\n",
       " (217, 1),\n",
       " (223, 1),\n",
       " (225, 1),\n",
       " (226, 2),\n",
       " (237, 1),\n",
       " (247, 1),\n",
       " (249, 1),\n",
       " (257, 1),\n",
       " (265, 2),\n",
       " (270, 5),\n",
       " (271, 1),\n",
       " (281, 1),\n",
       " (306, 1),\n",
       " (324, 3),\n",
       " (351, 3),\n",
       " (371, 1),\n",
       " (380, 1),\n",
       " (381, 1),\n",
       " (408, 1),\n",
       " (417, 1),\n",
       " (420, 1),\n",
       " (423, 1),\n",
       " (455, 1),\n",
       " (457, 3),\n",
       " (459, 1),\n",
       " (460, 1),\n",
       " (478, 1),\n",
       " (479, 1),\n",
       " (517, 1),\n",
       " (520, 3),\n",
       " (523, 1),\n",
       " (535, 1),\n",
       " (561, 1),\n",
       " (570, 4),\n",
       " (610, 2),\n",
       " (619, 1),\n",
       " (653, 1),\n",
       " (656, 1),\n",
       " (670, 1),\n",
       " (688, 1),\n",
       " (692, 1),\n",
       " (711, 2),\n",
       " (754, 2),\n",
       " (761, 1),\n",
       " (778, 1),\n",
       " (779, 1),\n",
       " (798, 1),\n",
       " (801, 2),\n",
       " (808, 2),\n",
       " (809, 1),\n",
       " (817, 1),\n",
       " (846, 1),\n",
       " (847, 1),\n",
       " (852, 2),\n",
       " (855, 1),\n",
       " (862, 1),\n",
       " (865, 1),\n",
       " (871, 3),\n",
       " (916, 1),\n",
       " (926, 1),\n",
       " (933, 1),\n",
       " (947, 1),\n",
       " (953, 1),\n",
       " (964, 1),\n",
       " (977, 1),\n",
       " (1010, 1),\n",
       " (1024, 1),\n",
       " (1039, 1),\n",
       " (1042, 1),\n",
       " (1084, 1),\n",
       " (1105, 1),\n",
       " (1138, 1),\n",
       " (1170, 1),\n",
       " (1186, 2),\n",
       " (1206, 4),\n",
       " (1207, 1),\n",
       " (1228, 1),\n",
       " (1229, 3),\n",
       " (1333, 1),\n",
       " (1354, 1),\n",
       " (1393, 2),\n",
       " (1434, 1),\n",
       " (1477, 1),\n",
       " (1486, 1),\n",
       " (1504, 1),\n",
       " (1513, 2),\n",
       " (1518, 1),\n",
       " (1537, 1),\n",
       " (1555, 3),\n",
       " (1578, 1),\n",
       " (1631, 1),\n",
       " (1676, 1),\n",
       " (1680, 2),\n",
       " (1683, 1),\n",
       " (1729, 1),\n",
       " (1731, 2),\n",
       " (1741, 1),\n",
       " (1758, 1),\n",
       " (1826, 1),\n",
       " (1851, 1),\n",
       " (1862, 2),\n",
       " (1910, 1),\n",
       " (1948, 1),\n",
       " (1964, 2),\n",
       " (2040, 1),\n",
       " (2067, 1),\n",
       " (2116, 1),\n",
       " (2179, 9),\n",
       " (2191, 2),\n",
       " (2233, 1),\n",
       " (2248, 3),\n",
       " (2367, 1),\n",
       " (2370, 1),\n",
       " (2375, 1),\n",
       " (2400, 1),\n",
       " (2433, 1),\n",
       " (2483, 1),\n",
       " (2547, 1),\n",
       " (2573, 2),\n",
       " (2618, 1),\n",
       " (2688, 1),\n",
       " (2704, 1),\n",
       " (2734, 1),\n",
       " (2769, 1),\n",
       " (2770, 1),\n",
       " (2779, 1),\n",
       " (2787, 1),\n",
       " (2796, 2),\n",
       " (2819, 2),\n",
       " (2839, 1),\n",
       " (2916, 1),\n",
       " (3021, 1),\n",
       " (3033, 1),\n",
       " (3047, 5),\n",
       " (3048, 1),\n",
       " (3052, 1),\n",
       " (3061, 1),\n",
       " (3170, 1),\n",
       " (3248, 2),\n",
       " (3330, 1),\n",
       " (3361, 2),\n",
       " (3369, 1),\n",
       " (3493, 1),\n",
       " (3563, 1),\n",
       " (3573, 1),\n",
       " (3594, 1),\n",
       " (3654, 1),\n",
       " (3670, 1),\n",
       " (3864, 1),\n",
       " (3872, 1),\n",
       " (3973, 1),\n",
       " (4130, 1),\n",
       " (4140, 2),\n",
       " (4196, 1),\n",
       " (4207, 1),\n",
       " (4215, 1),\n",
       " (4420, 1),\n",
       " (4432, 1),\n",
       " (4619, 1),\n",
       " (4652, 1),\n",
       " (4707, 1),\n",
       " (4766, 1),\n",
       " (4770, 1),\n",
       " (4949, 1),\n",
       " (5195, 1),\n",
       " (5541, 1),\n",
       " (5574, 1),\n",
       " (5595, 1),\n",
       " (6082, 1),\n",
       " (6093, 1),\n",
       " (6210, 1),\n",
       " (6418, 1),\n",
       " (6424, 1),\n",
       " (6818, 1),\n",
       " (7077, 1),\n",
       " (7175, 1),\n",
       " (7197, 1),\n",
       " (7458, 1),\n",
       " (7512, 1),\n",
       " (7904, 1),\n",
       " (8318, 1),\n",
       " (8414, 1),\n",
       " (8426, 1),\n",
       " (8558, 1),\n",
       " (8909, 1),\n",
       " (8971, 1),\n",
       " (9649, 1),\n",
       " (9825, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 (\"case\") appears 1 time.\n",
      "Word 4 (\"come\") appears 1 time.\n",
      "Word 5 (\"commit\") appears 1 time.\n",
      "Word 14 (\"like\") appears 1 time.\n",
      "Word 15 (\"million\") appears 2 time.\n",
      "Word 24 (\"relat\") appears 1 time.\n",
      "Word 36 (\"american\") appears 1 time.\n",
      "Word 79 (\"inform\") appears 1 time.\n",
      "Word 106 (\"public\") appears 2 time.\n",
      "Word 119 (\"state\") appears 1 time.\n",
      "Word 125 (\"world\") appears 4 time.\n",
      "Word 126 (\"write\") appears 3 time.\n",
      "Word 132 (\"base\") appears 1 time.\n",
      "Word 134 (\"break\") appears 1 time.\n",
      "Word 151 (\"form\") appears 1 time.\n",
      "Word 152 (\"give\") appears 1 time.\n",
      "Word 160 (\"know\") appears 1 time.\n",
      "Word 167 (\"news\") appears 1 time.\n",
      "Word 191 (\"donald\") appears 2 time.\n",
      "Word 195 (\"great\") appears 1 time.\n",
      "Word 199 (\"presid\") appears 1 time.\n",
      "Word 206 (\"trump\") appears 2 time.\n",
      "Word 211 (\"america\") appears 2 time.\n",
      "Word 214 (\"bring\") appears 1 time.\n",
      "Word 216 (\"busi\") appears 1 time.\n",
      "Word 217 (\"care\") appears 1 time.\n",
      "Word 223 (\"complet\") appears 1 time.\n",
      "Word 225 (\"cost\") appears 1 time.\n",
      "Word 226 (\"countri\") appears 2 time.\n",
      "Word 237 (\"elect\") appears 1 time.\n",
      "Word 247 (\"higher\") appears 1 time.\n",
      "Word 249 (\"hous\") appears 1 time.\n",
      "Word 257 (\"long\") appears 1 time.\n",
      "Word 265 (\"opportun\") appears 2 time.\n",
      "Word 270 (\"peopl\") appears 5 time.\n",
      "Word 271 (\"person\") appears 1 time.\n",
      "Word 281 (\"repres\") appears 1 time.\n",
      "Word 306 (\"work\") appears 1 time.\n",
      "Word 324 (\"go\") appears 3 time.\n",
      "Word 351 (\"start\") appears 3 time.\n",
      "Word 371 (\"away\") appears 1 time.\n",
      "Word 380 (\"citi\") appears 1 time.\n",
      "Word 381 (\"claim\") appears 1 time.\n",
      "Word 408 (\"differ\") appears 1 time.\n",
      "Word 417 (\"event\") appears 1 time.\n",
      "Word 420 (\"expos\") appears 1 time.\n",
      "Word 423 (\"fact\") appears 1 time.\n",
      "Word 455 (\"knowledg\") appears 1 time.\n",
      "Word 457 (\"level\") appears 3 time.\n",
      "Word 459 (\"look\") appears 1 time.\n",
      "Word 460 (\"make\") appears 1 time.\n",
      "Word 478 (\"point\") appears 1 time.\n",
      "Word 479 (\"polic\") appears 1 time.\n",
      "Word 517 (\"tactic\") appears 1 time.\n",
      "Word 520 (\"think\") appears 3 time.\n",
      "Word 523 (\"time\") appears 1 time.\n",
      "Word 535 (\"week\") appears 1 time.\n",
      "Word 561 (\"earli\") appears 1 time.\n",
      "Word 570 (\"live\") appears 4 time.\n",
      "Word 610 (\"chang\") appears 2 time.\n",
      "Word 619 (\"larg\") appears 1 time.\n",
      "Word 653 (\"take\") appears 1 time.\n",
      "Word 656 (\"white\") appears 1 time.\n",
      "Word 670 (\"call\") appears 1 time.\n",
      "Word 688 (\"fall\") appears 1 time.\n",
      "Word 692 (\"good\") appears 1 time.\n",
      "Word 711 (\"learn\") appears 2 time.\n",
      "Word 754 (\"view\") appears 2 time.\n",
      "Word 761 (\"academ\") appears 1 time.\n",
      "Word 778 (\"colleg\") appears 1 time.\n",
      "Word 779 (\"communiti\") appears 1 time.\n",
      "Word 798 (\"express\") appears 1 time.\n",
      "Word 801 (\"fear\") appears 2 time.\n",
      "Word 808 (\"high\") appears 2 time.\n",
      "Word 809 (\"histor\") appears 1 time.\n",
      "Word 817 (\"intellectu\") appears 1 time.\n",
      "Word 846 (\"research\") appears 1 time.\n",
      "Word 847 (\"resourc\") appears 1 time.\n",
      "Word 852 (\"school\") appears 2 time.\n",
      "Word 855 (\"shape\") appears 1 time.\n",
      "Word 862 (\"teach\") appears 1 time.\n",
      "Word 865 (\"today\") appears 1 time.\n",
      "Word 871 (\"valu\") appears 3 time.\n",
      "Word 916 (\"media\") appears 1 time.\n",
      "Word 926 (\"pennsylvania\") appears 1 time.\n",
      "Word 933 (\"reveal\") appears 1 time.\n",
      "Word 947 (\"virginia\") appears 1 time.\n",
      "Word 953 (\"liberti\") appears 1 time.\n",
      "Word 964 (\"violenc\") appears 1 time.\n",
      "Word 977 (\"billionair\") appears 1 time.\n",
      "Word 1010 (\"major\") appears 1 time.\n",
      "Word 1024 (\"read\") appears 1 time.\n",
      "Word 1039 (\"town\") appears 1 time.\n",
      "Word 1042 (\"unit\") appears 1 time.\n",
      "Word 1084 (\"portray\") appears 1 time.\n",
      "Word 1105 (\"count\") appears 1 time.\n",
      "Word 1138 (\"risk\") appears 1 time.\n",
      "Word 1170 (\"financi\") appears 1 time.\n",
      "Word 1186 (\"chicago\") appears 2 time.\n",
      "Word 1206 (\"daili\") appears 4 time.\n",
      "Word 1207 (\"destroy\") appears 1 time.\n",
      "Word 1228 (\"mogul\") appears 1 time.\n",
      "Word 1229 (\"moral\") appears 3 time.\n",
      "Word 1333 (\"defend\") appears 1 time.\n",
      "Word 1354 (\"sound\") appears 1 time.\n",
      "Word 1393 (\"reason\") appears 2 time.\n",
      "Word 1434 (\"west\") appears 1 time.\n",
      "Word 1477 (\"race\") appears 1 time.\n",
      "Word 1486 (\"signific\") appears 1 time.\n",
      "Word 1504 (\"begin\") appears 1 time.\n",
      "Word 1513 (\"caus\") appears 2 time.\n",
      "Word 1518 (\"contribut\") appears 1 time.\n",
      "Word 1537 (\"inner\") appears 1 time.\n",
      "Word 1555 (\"pay\") appears 3 time.\n",
      "Word 1578 (\"text\") appears 1 time.\n",
      "Word 1631 (\"name\") appears 1 time.\n",
      "Word 1676 (\"abil\") appears 1 time.\n",
      "Word 1680 (\"adult\") appears 2 time.\n",
      "Word 1683 (\"agent\") appears 1 time.\n",
      "Word 1729 (\"callous\") appears 1 time.\n",
      "Word 1731 (\"career\") appears 2 time.\n",
      "Word 1741 (\"climat\") appears 1 time.\n",
      "Word 1758 (\"consequ\") appears 1 time.\n",
      "Word 1826 (\"escal\") appears 1 time.\n",
      "Word 1851 (\"frustrat\") appears 1 time.\n",
      "Word 1862 (\"grow\") appears 2 time.\n",
      "Word 1910 (\"lack\") appears 1 time.\n",
      "Word 1948 (\"motiv\") appears 1 time.\n",
      "Word 1964 (\"opinion\") appears 2 time.\n",
      "Word 2040 (\"self\") appears 1 time.\n",
      "Word 2067 (\"struggl\") appears 1 time.\n",
      "Word 2116 (\"voic\") appears 1 time.\n",
      "Word 2179 (\"common\") appears 9 time.\n",
      "Word 2191 (\"decis\") appears 2 time.\n",
      "Word 2233 (\"guardian\") appears 1 time.\n",
      "Word 2248 (\"job\") appears 3 time.\n",
      "Word 2367 (\"tax\") appears 1 time.\n",
      "Word 2370 (\"thrive\") appears 1 time.\n",
      "Word 2375 (\"trade\") appears 1 time.\n",
      "Word 2400 (\"factor\") appears 1 time.\n",
      "Word 2433 (\"equat\") appears 1 time.\n",
      "Word 2483 (\"resum\") appears 1 time.\n",
      "Word 2547 (\"hoax\") appears 1 time.\n",
      "Word 2573 (\"cultur\") appears 2 time.\n",
      "Word 2618 (\"misogyni\") appears 1 time.\n",
      "Word 2688 (\"enjoy\") appears 1 time.\n",
      "Word 2704 (\"vent\") appears 1 time.\n",
      "Word 2734 (\"defin\") appears 1 time.\n",
      "Word 2769 (\"limit\") appears 1 time.\n",
      "Word 2770 (\"local\") appears 1 time.\n",
      "Word 2779 (\"obscur\") appears 1 time.\n",
      "Word 2787 (\"proper\") appears 1 time.\n",
      "Word 2796 (\"safe\") appears 2 time.\n",
      "Word 2819 (\"young\") appears 2 time.\n",
      "Word 2839 (\"small\") appears 1 time.\n",
      "Word 2916 (\"dead\") appears 1 time.\n",
      "Word 3021 (\"poor\") appears 1 time.\n",
      "Word 3033 (\"reader\") appears 1 time.\n",
      "Word 3047 (\"sens\") appears 5 time.\n",
      "Word 3048 (\"seventi\") appears 1 time.\n",
      "Word 3052 (\"simpl\") appears 1 time.\n",
      "Word 3061 (\"stamp\") appears 1 time.\n",
      "Word 3170 (\"attend\") appears 1 time.\n",
      "Word 3248 (\"applic\") appears 2 time.\n",
      "Word 3330 (\"employ\") appears 1 time.\n",
      "Word 3361 (\"function\") appears 2 time.\n",
      "Word 3369 (\"graduat\") appears 1 time.\n",
      "Word 3493 (\"suicid\") appears 1 time.\n",
      "Word 3563 (\"hire\") appears 1 time.\n",
      "Word 3573 (\"newspap\") appears 1 time.\n",
      "Word 3594 (\"transfer\") appears 1 time.\n",
      "Word 3654 (\"eas\") appears 1 time.\n",
      "Word 3670 (\"last\") appears 1 time.\n",
      "Word 3864 (\"food\") appears 1 time.\n",
      "Word 3872 (\"heinous\") appears 1 time.\n",
      "Word 3973 (\"millionair\") appears 1 time.\n",
      "Word 4130 (\"dictionari\") appears 1 time.\n",
      "Word 4140 (\"judgement\") appears 2 time.\n",
      "Word 4196 (\"mine\") appears 1 time.\n",
      "Word 4207 (\"everyday\") appears 1 time.\n",
      "Word 4215 (\"grade\") appears 1 time.\n",
      "Word 4420 (\"idioci\") appears 1 time.\n",
      "Word 4432 (\"folk\") appears 1 time.\n",
      "Word 4619 (\"landscap\") appears 1 time.\n",
      "Word 4652 (\"anxieti\") appears 1 time.\n",
      "Word 4707 (\"youth\") appears 1 time.\n",
      "Word 4766 (\"communism\") appears 1 time.\n",
      "Word 4770 (\"dumb\") appears 1 time.\n",
      "Word 4949 (\"cake\") appears 1 time.\n",
      "Word 5195 (\"prevail\") appears 1 time.\n",
      "Word 5541 (\"categori\") appears 1 time.\n",
      "Word 5574 (\"disrespect\") appears 1 time.\n",
      "Word 5595 (\"eighti\") appears 1 time.\n",
      "Word 6082 (\"arbitr\") appears 1 time.\n",
      "Word 6093 (\"coal\") appears 1 time.\n",
      "Word 6210 (\"nutshel\") appears 1 time.\n",
      "Word 6418 (\"charlatan\") appears 1 time.\n",
      "Word 6424 (\"countless\") appears 1 time.\n",
      "Word 6818 (\"plagu\") appears 1 time.\n",
      "Word 7077 (\"canon\") appears 1 time.\n",
      "Word 7175 (\"cambridg\") appears 1 time.\n",
      "Word 7197 (\"lesson\") appears 1 time.\n",
      "Word 7458 (\"modest\") appears 1 time.\n",
      "Word 7512 (\"brightest\") appears 1 time.\n",
      "Word 7904 (\"senseless\") appears 1 time.\n",
      "Word 8318 (\"instig\") appears 1 time.\n",
      "Word 8414 (\"limb\") appears 1 time.\n",
      "Word 8426 (\"mccarthyism\") appears 1 time.\n",
      "Word 8558 (\"candl\") appears 1 time.\n",
      "Word 8909 (\"sixti\") appears 1 time.\n",
      "Word 8971 (\"diploma\") appears 1 time.\n",
      "Word 9649 (\"irrevers\") appears 1 time.\n",
      "Word 9825 (\"sporad\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                                     dictionary[bow_doc_4310[i][0]], \n",
    "                                                     bow_doc_4310[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3500512547230274),\n",
      " (1, 0.14779052811571117),\n",
      " (2, 0.2509550132368066),\n",
      " (3, 0.0959276381032166),\n",
      " (4, 0.05650432146213837),\n",
      " (5, 0.13795848967793725),\n",
      " (6, 0.09847959217082528),\n",
      " (7, 0.23881625132641596),\n",
      " (8, 0.11286182174525332),\n",
      " (9, 0.11115771642046168),\n",
      " (10, 0.16957916134677647),\n",
      " (11, 0.14508239585282315),\n",
      " (12, 0.18489604253976888),\n",
      " (13, 0.16170327437243945),\n",
      " (14, 0.04460067252936085),\n",
      " (15, 0.09505506146952088),\n",
      " (16, 0.11241507548990382),\n",
      " (17, 0.09590387754528427),\n",
      " (18, 0.16072623307835415),\n",
      " (19, 0.07263567932123731),\n",
      " (20, 0.17496626609002672),\n",
      " (21, 0.19336028975000125),\n",
      " (22, 0.15940061131640726),\n",
      " (23, 0.19077058991862686),\n",
      " (24, 0.09364517477467808),\n",
      " (25, 0.060942405994516446),\n",
      " (26, 0.2583702830532261),\n",
      " (27, 0.3500512547230274),\n",
      " (28, 0.33623480159032987),\n",
      " (29, 0.21990575284339772),\n",
      " (30, 0.04787518694858638)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.011*\"elect\" + 0.011*\"trump\" + 0.011*\"state\" + 0.010*\"vote\" + 0.009*\"clinton\" + 0.006*\"peopl\" + 0.005*\"presid\" + 0.005*\"voter\" + 0.005*\"obama\" + 0.004*\"time\"\n",
      "Topic: 1 \n",
      "Words: 0.007*\"bank\" + 0.006*\"clinton\" + 0.006*\"world\" + 0.006*\"state\" + 0.005*\"gold\" + 0.005*\"money\" + 0.005*\"peopl\" + 0.004*\"trump\" + 0.004*\"year\" + 0.004*\"nation\"\n",
      "Topic: 2 \n",
      "Words: 0.024*\"clinton\" + 0.014*\"hillari\" + 0.011*\"email\" + 0.007*\"investig\" + 0.006*\"campaign\" + 0.006*\"state\" + 0.005*\"report\" + 0.005*\"presid\" + 0.004*\"trump\" + 0.004*\"elect\"\n",
      "Topic: 3 \n",
      "Words: 0.006*\"report\" + 0.005*\"year\" + 0.004*\"forc\" + 0.004*\"peopl\" + 0.004*\"time\" + 0.004*\"like\" + 0.004*\"know\" + 0.004*\"militari\" + 0.003*\"citi\" + 0.003*\"state\"\n",
      "Topic: 4 \n",
      "Words: 0.009*\"peopl\" + 0.007*\"like\" + 0.006*\"world\" + 0.005*\"year\" + 0.005*\"time\" + 0.004*\"state\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"american\" + 0.004*\"nation\"\n",
      "Topic: 5 \n",
      "Words: 0.007*\"peopl\" + 0.006*\"state\" + 0.006*\"govern\" + 0.004*\"year\" + 0.004*\"pipelin\" + 0.004*\"world\" + 0.004*\"time\" + 0.004*\"water\" + 0.004*\"know\" + 0.004*\"american\"\n",
      "Topic: 6 \n",
      "Words: 0.006*\"time\" + 0.005*\"like\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"work\" + 0.004*\"know\" + 0.003*\"food\" + 0.003*\"bodi\" + 0.003*\"help\" + 0.003*\"video\"\n",
      "Topic: 7 \n",
      "Words: 0.009*\"trump\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"peopl\" + 0.006*\"think\" + 0.005*\"news\" + 0.005*\"clinton\" + 0.005*\"know\" + 0.005*\"state\" + 0.004*\"hillari\"\n",
      "Topic: 8 \n",
      "Words: 0.018*\"trump\" + 0.008*\"elect\" + 0.007*\"american\" + 0.007*\"clinton\" + 0.007*\"presid\" + 0.006*\"peopl\" + 0.006*\"russia\" + 0.006*\"state\" + 0.006*\"govern\" + 0.006*\"syria\"\n",
      "Topic: 9 \n",
      "Words: 0.009*\"forc\" + 0.009*\"russian\" + 0.008*\"russia\" + 0.005*\"state\" + 0.004*\"nato\" + 0.004*\"countri\" + 0.004*\"infowar\" + 0.004*\"like\" + 0.004*\"brain\" + 0.004*\"militari\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"clinton\" + 0.002*\"trump\" + 0.002*\"email\" + 0.002*\"hillari\" + 0.001*\"report\" + 0.001*\"state\" + 0.001*\"elect\" + 0.001*\"peopl\" + 0.001*\"year\" + 0.001*\"facebook\"\n",
      "Topic: 1 Word: 0.006*\"clinton\" + 0.006*\"email\" + 0.005*\"investig\" + 0.004*\"hillari\" + 0.004*\"comey\" + 0.003*\"weiner\" + 0.003*\"trump\" + 0.002*\"abedin\" + 0.002*\"elect\" + 0.002*\"server\"\n",
      "Topic: 2 Word: 0.003*\"dakota\" + 0.003*\"protest\" + 0.003*\"pipelin\" + 0.002*\"polic\" + 0.002*\"rock\" + 0.002*\"trump\" + 0.002*\"stand\" + 0.002*\"sioux\" + 0.002*\"sheriff\" + 0.002*\"arrest\"\n",
      "Topic: 3 Word: 0.003*\"mosul\" + 0.003*\"syria\" + 0.003*\"aleppo\" + 0.003*\"syrian\" + 0.003*\"isi\" + 0.002*\"trump\" + 0.002*\"iraqi\" + 0.002*\"russian\" + 0.002*\"clinton\" + 0.002*\"terrorist\"\n",
      "Topic: 4 Word: 0.003*\"trump\" + 0.003*\"vote\" + 0.002*\"clinton\" + 0.002*\"elect\" + 0.002*\"obama\" + 0.002*\"hillari\" + 0.002*\"state\" + 0.001*\"report\" + 0.001*\"presid\" + 0.001*\"voter\"\n",
      "Topic: 5 Word: 0.004*\"trump\" + 0.003*\"clinton\" + 0.003*\"moreno\" + 0.002*\"hillari\" + 0.002*\"bundi\" + 0.002*\"elect\" + 0.002*\"vote\" + 0.002*\"american\" + 0.001*\"donald\" + 0.001*\"presid\"\n",
      "Topic: 6 Word: 0.003*\"trump\" + 0.002*\"clinton\" + 0.002*\"stockman\" + 0.002*\"david\" + 0.002*\"guest\" + 0.002*\"hillari\" + 0.002*\"vote\" + 0.001*\"elect\" + 0.001*\"email\" + 0.001*\"state\"\n",
      "Topic: 7 Word: 0.005*\"trump\" + 0.003*\"clinton\" + 0.003*\"elect\" + 0.002*\"hillari\" + 0.002*\"vote\" + 0.002*\"russia\" + 0.002*\"american\" + 0.002*\"presid\" + 0.002*\"state\" + 0.002*\"peopl\"\n",
      "Topic: 8 Word: 0.007*\"text\" + 0.004*\"trump\" + 0.003*\"clinton\" + 0.003*\"hillari\" + 0.003*\"http\" + 0.002*\"russia\" + 0.002*\"result\" + 0.002*\"vote\" + 0.002*\"nato\" + 0.002*\"elect\"\n",
      "Topic: 9 Word: 0.003*\"clinton\" + 0.003*\"trump\" + 0.002*\"hillari\" + 0.002*\"email\" + 0.001*\"elect\" + 0.001*\"russia\" + 0.001*\"investig\" + 0.001*\"comey\" + 0.001*\"campaign\" + 0.001*\"vote\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['common',\n",
       " 'sens',\n",
       " 'defin',\n",
       " 'cambridg',\n",
       " 'academ',\n",
       " 'text',\n",
       " 'dictionari',\n",
       " 'abil',\n",
       " 'good',\n",
       " 'judgement',\n",
       " 'make',\n",
       " 'decis',\n",
       " 'live',\n",
       " 'reason',\n",
       " 'safe',\n",
       " 'sound',\n",
       " 'reason',\n",
       " 'safe',\n",
       " 'sixti',\n",
       " 'seventi',\n",
       " 'grow',\n",
       " 'risk',\n",
       " 'daili',\n",
       " 'base',\n",
       " 'judgement',\n",
       " 'decis',\n",
       " 'last',\n",
       " 'irrevers',\n",
       " 'consequ',\n",
       " 'start',\n",
       " 'write',\n",
       " 'sporad',\n",
       " 'begin',\n",
       " 'earli',\n",
       " 'eighti',\n",
       " 'small',\n",
       " 'newspap',\n",
       " 'like',\n",
       " 'chicago',\n",
       " 'daili',\n",
       " 'news',\n",
       " 'chicago',\n",
       " 'defend',\n",
       " 'enjoy',\n",
       " 'opportun',\n",
       " 'research',\n",
       " 'expos',\n",
       " 'resourc',\n",
       " 'start',\n",
       " 'canon',\n",
       " 'career',\n",
       " 'week',\n",
       " 'contribut',\n",
       " 'reader',\n",
       " 'look',\n",
       " 'career',\n",
       " 'employ',\n",
       " 'opportun',\n",
       " 'financi',\n",
       " 'case',\n",
       " 'pay',\n",
       " 'attend',\n",
       " 'colleg',\n",
       " 'start',\n",
       " 'say',\n",
       " 'guardian',\n",
       " 'liberti',\n",
       " 'voic',\n",
       " 'motiv',\n",
       " 'express',\n",
       " 'person',\n",
       " 'opinion',\n",
       " 'world',\n",
       " 'limit',\n",
       " 'world',\n",
       " 'view',\n",
       " 'local',\n",
       " 'landscap',\n",
       " 'travail',\n",
       " 'everyday',\n",
       " 'world',\n",
       " 'larg',\n",
       " 'portray',\n",
       " 'media',\n",
       " 'mogul',\n",
       " 'high',\n",
       " 'pay',\n",
       " 'infotain',\n",
       " 'charlatan',\n",
       " 'write',\n",
       " 'vent',\n",
       " 'frustrat',\n",
       " 'fear',\n",
       " 'world',\n",
       " 'dumb',\n",
       " 'point',\n",
       " 'common',\n",
       " 'sens',\n",
       " 'common',\n",
       " 'grow',\n",
       " 'public',\n",
       " 'hous',\n",
       " 'go',\n",
       " 'public',\n",
       " 'school',\n",
       " 'think',\n",
       " 'intellectu',\n",
       " 'brightest',\n",
       " 'candl',\n",
       " 'cake',\n",
       " 'think',\n",
       " 'fall',\n",
       " 'level',\n",
       " 'illiteraci',\n",
       " 'adult',\n",
       " 'major',\n",
       " 'inner',\n",
       " 'citi',\n",
       " 'youth',\n",
       " 'young',\n",
       " 'adult',\n",
       " 'struggl',\n",
       " 'read',\n",
       " 'write',\n",
       " 'grade',\n",
       " 'level',\n",
       " 'higher',\n",
       " 'busi',\n",
       " 'hire',\n",
       " 'peopl',\n",
       " 'daili',\n",
       " 'peopl',\n",
       " 'work',\n",
       " 'daili',\n",
       " 'proper',\n",
       " 'applic',\n",
       " 'transfer',\n",
       " 'inform',\n",
       " 'resum',\n",
       " 'applic',\n",
       " 'fact',\n",
       " 'illiteraci',\n",
       " 'break',\n",
       " 'categori',\n",
       " 'function',\n",
       " 'cultur',\n",
       " 'moral',\n",
       " 'eas',\n",
       " 'anxieti',\n",
       " 'relat',\n",
       " 'modest',\n",
       " 'simpl',\n",
       " 'differ',\n",
       " 'opinion',\n",
       " 'escal',\n",
       " 'level',\n",
       " 'violenc',\n",
       " 'idioci',\n",
       " 'cost',\n",
       " 'peopl',\n",
       " 'limb',\n",
       " 'live',\n",
       " 'function',\n",
       " 'illiteraci',\n",
       " 'plagu',\n",
       " 'million',\n",
       " 'american',\n",
       " 'million',\n",
       " 'graduat',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " 'valu',\n",
       " 'food',\n",
       " 'stamp',\n",
       " 'cultur',\n",
       " 'illiteraci',\n",
       " 'lack',\n",
       " 'knowledg',\n",
       " 'signific',\n",
       " 'histor',\n",
       " 'event',\n",
       " 'shape',\n",
       " 'countri',\n",
       " 'obscur',\n",
       " 'lesson',\n",
       " 'learn',\n",
       " 'learn',\n",
       " 'moral',\n",
       " 'illiteraci',\n",
       " 'complet',\n",
       " 'form',\n",
       " 'illiteraci',\n",
       " 'view',\n",
       " 'caus',\n",
       " 'senseless',\n",
       " 'heinous',\n",
       " 'callous',\n",
       " 'prevail',\n",
       " 'unit',\n",
       " 'state',\n",
       " 'today',\n",
       " 'young',\n",
       " 'teach',\n",
       " 'moral',\n",
       " 'valu',\n",
       " 'disrespect',\n",
       " 'valu',\n",
       " 'care',\n",
       " 'self',\n",
       " 'communiti',\n",
       " 'say',\n",
       " 'common',\n",
       " 'sens',\n",
       " 'factor',\n",
       " 'equat',\n",
       " 'common',\n",
       " 'sens',\n",
       " 'count',\n",
       " 'say',\n",
       " 'fella',\n",
       " 'live',\n",
       " 'countri',\n",
       " 'elect',\n",
       " 'name',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'presid',\n",
       " 'say',\n",
       " 'repres',\n",
       " 'peopl',\n",
       " 'chang',\n",
       " 'agent',\n",
       " 'go',\n",
       " 'america',\n",
       " 'great',\n",
       " 'go',\n",
       " 'bring',\n",
       " 'job',\n",
       " 'poor',\n",
       " 'trade',\n",
       " 'polic',\n",
       " 'give',\n",
       " 'away',\n",
       " 'say',\n",
       " 'climat',\n",
       " 'chang',\n",
       " 'hoax',\n",
       " 'coal',\n",
       " 'mine',\n",
       " 'job',\n",
       " 'come',\n",
       " 'west',\n",
       " 'virginia',\n",
       " 'pennsylvania',\n",
       " 'lili',\n",
       " 'white',\n",
       " 'town',\n",
       " 'thrive',\n",
       " 'dead',\n",
       " 'pay',\n",
       " 'job',\n",
       " 'proteg',\n",
       " 'cohn',\n",
       " 'instig',\n",
       " 'arbitr',\n",
       " 'mccarthyism',\n",
       " 'caus',\n",
       " 'peopl',\n",
       " 'commit',\n",
       " 'suicid',\n",
       " 'destroy',\n",
       " 'countless',\n",
       " 'live',\n",
       " 'tactic',\n",
       " 'fear',\n",
       " 'race',\n",
       " 'misogyni',\n",
       " 'communism',\n",
       " 'nutshel',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'cohn',\n",
       " 'america',\n",
       " 'call',\n",
       " 'common',\n",
       " 'folk',\n",
       " 'think',\n",
       " 'millionair',\n",
       " 'billionair',\n",
       " 'know',\n",
       " 'reveal',\n",
       " 'tax',\n",
       " 'claim',\n",
       " 'common',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'common',\n",
       " 'sens',\n",
       " 'common']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5360153317451477\t \n",
      "Topic: 0.009*\"peopl\" + 0.007*\"like\" + 0.006*\"world\" + 0.005*\"year\" + 0.005*\"time\" + 0.004*\"state\" + 0.004*\"go\" + 0.004*\"know\" + 0.004*\"american\" + 0.004*\"nation\"\n",
      "\n",
      "Score: 0.17714190483093262\t \n",
      "Topic: 0.007*\"peopl\" + 0.006*\"state\" + 0.006*\"govern\" + 0.004*\"year\" + 0.004*\"pipelin\" + 0.004*\"world\" + 0.004*\"time\" + 0.004*\"water\" + 0.004*\"know\" + 0.004*\"american\"\n",
      "\n",
      "Score: 0.11762698739767075\t \n",
      "Topic: 0.006*\"time\" + 0.005*\"like\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"work\" + 0.004*\"know\" + 0.003*\"food\" + 0.003*\"bodi\" + 0.003*\"help\" + 0.003*\"video\"\n",
      "\n",
      "Score: 0.09282205253839493\t \n",
      "Topic: 0.018*\"trump\" + 0.008*\"elect\" + 0.007*\"american\" + 0.007*\"clinton\" + 0.007*\"presid\" + 0.006*\"peopl\" + 0.006*\"russia\" + 0.006*\"state\" + 0.006*\"govern\" + 0.006*\"syria\"\n",
      "\n",
      "Score: 0.04674949124455452\t \n",
      "Topic: 0.011*\"elect\" + 0.011*\"trump\" + 0.011*\"state\" + 0.010*\"vote\" + 0.009*\"clinton\" + 0.006*\"peopl\" + 0.005*\"presid\" + 0.005*\"voter\" + 0.005*\"obama\" + 0.004*\"time\"\n",
      "\n",
      "Score: 0.0201182272285223\t \n",
      "Topic: 0.009*\"trump\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"peopl\" + 0.006*\"think\" + 0.005*\"news\" + 0.005*\"clinton\" + 0.005*\"know\" + 0.005*\"state\" + 0.004*\"hillari\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9714866280555725\t \n",
      "Topic: 0.005*\"trump\" + 0.003*\"clinton\" + 0.003*\"elect\" + 0.002*\"hillari\" + 0.002*\"vote\" + 0.002*\"russia\" + 0.002*\"american\" + 0.002*\"presid\" + 0.002*\"state\" + 0.002*\"peopl\"\n",
      "\n",
      "Score: 0.02568981982767582\t \n",
      "Topic: 0.003*\"clinton\" + 0.002*\"trump\" + 0.002*\"email\" + 0.002*\"hillari\" + 0.001*\"report\" + 0.001*\"state\" + 0.001*\"elect\" + 0.001*\"peopl\" + 0.001*\"year\" + 0.001*\"facebook\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8498435616493225\t Topic: 0.018*\"trump\" + 0.008*\"elect\" + 0.007*\"american\" + 0.007*\"clinton\" + 0.007*\"presid\"\n",
      "Score: 0.016692448407411575\t Topic: 0.011*\"elect\" + 0.011*\"trump\" + 0.011*\"state\" + 0.010*\"vote\" + 0.009*\"clinton\"\n",
      "Score: 0.01668519340455532\t Topic: 0.009*\"trump\" + 0.007*\"time\" + 0.006*\"like\" + 0.006*\"peopl\" + 0.006*\"think\"\n",
      "Score: 0.016684705391526222\t Topic: 0.006*\"time\" + 0.005*\"like\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"work\"\n",
      "Score: 0.01668364182114601\t Topic: 0.007*\"peopl\" + 0.006*\"state\" + 0.006*\"govern\" + 0.004*\"year\" + 0.004*\"pipelin\"\n",
      "Score: 0.01668323017656803\t Topic: 0.024*\"clinton\" + 0.014*\"hillari\" + 0.011*\"email\" + 0.007*\"investig\" + 0.006*\"campaign\"\n",
      "Score: 0.016682615503668785\t Topic: 0.007*\"bank\" + 0.006*\"clinton\" + 0.006*\"world\" + 0.006*\"state\" + 0.005*\"gold\"\n",
      "Score: 0.016682488843798637\t Topic: 0.009*\"peopl\" + 0.007*\"like\" + 0.006*\"world\" + 0.005*\"year\" + 0.005*\"time\"\n",
      "Score: 0.016681866720318794\t Topic: 0.009*\"forc\" + 0.009*\"russian\" + 0.008*\"russia\" + 0.005*\"state\" + 0.004*\"nato\"\n",
      "Score: 0.016680225729942322\t Topic: 0.006*\"report\" + 0.005*\"year\" + 0.004*\"forc\" + 0.004*\"peopl\" + 0.004*\"time\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
